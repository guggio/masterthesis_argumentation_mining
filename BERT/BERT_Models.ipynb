{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1PMyqkc3quwO",
        "8-NrhYj7qyDE",
        "cPogG9KLk-Mv",
        "YG8toIFclN2F",
        "So5zmYgHAAq7",
        "Na4-y5q5-m6b",
        "mU0pK2nP-m6x",
        "q_D45FAbq50R",
        "q7Fj7zuwT4Wo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwTn4BXrqsYU",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation of BERT Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCHAK7E0rMOz",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWFcXiG_rR5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNWhGFyMrVUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split, KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqKTvJzuxg4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/deepset-ai/FARM.git\n",
        "!pip install -r FARM/requirements.txt\n",
        "!pip install FARM/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHPV8RrCxpdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from farm.modeling.tokenization import BertTokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import Bert\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.data_handler.processor import NERProcessor\n",
        "from farm.modeling.prediction_head import TokenClassificationHead\n",
        "from farm.infer import Inferencer\n",
        "from pprint import PrettyPrinter\n",
        "\n",
        "#Load language model\n",
        "MODEL_NAME_OR_PATH = \"bert-base-german-cased\"\n",
        "language_model = Bert.load(MODEL_NAME_OR_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly634l6_xzRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Devices available: {}\".format(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9pHBHHArcRr",
        "colab_type": "text"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJIiiSYwrfKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = F\"/content/gdrive/My Drive/Colab Notebooks/Corpus/full_annotations_v10.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6OlW9XawC_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(dataset_path)\n",
        "reviews = list(set(df['Review'].values))\n",
        "reviews.sort(key=lambda x: int(x[6:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jdd0SGePHqnA"
      },
      "source": [
        "## Component Identification & Classification combined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mc2rzIBrHqm-",
        "colab": {}
      },
      "source": [
        "#New Preparation of the dataset\n",
        "df_new = df\n",
        "\n",
        "z='O'\n",
        "combined_labels = list()\n",
        "for i in range(len(df_new)):\n",
        "  if df_new['BIO'].iloc[i] == 'B':\n",
        "    z = df_new['Ann_Ann'].iloc[i]\n",
        "    combined_labels.append('B-' + z)\n",
        "  elif df_new['BIO'].iloc[i] == 'I':\n",
        "    combined_labels.append('I-' + z)\n",
        "  else:\n",
        "    z = 'O'\n",
        "    combined_labels.append('B-' + z)\n",
        "\n",
        "df_new['combined_labels'] = combined_labels\n",
        "\n",
        "#Split of the train and test data\n",
        "reviews_train, reviews_test = train_test_split(reviews, test_size=0.2, random_state=42)\n",
        "df_new_train = df_new[df_new['Review'].isin(reviews_train)]\n",
        "df_new_test = df_new[df_new['Review'].isin(reviews_test)]\n",
        "\n",
        "print(df_new.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LRVSxyAOHqm8",
        "colab": {}
      },
      "source": [
        "f_train = open('train.txt', 'w+')\n",
        "for rev in reviews_train:\n",
        "  df_relevant = df_new_train[df_new_train['Review'] == rev]\n",
        "  for i in range(len(df_relevant)):\n",
        "    f_train.write(str(df_relevant['Tokens'].iloc[i]) + '\\t' + str(df_relevant['combined_labels'].iloc[i] + '\\n'))\n",
        " \n",
        "  f_train.write('\\n')\n",
        "f_train.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZNbJ9_GLHqm5",
        "colab": {}
      },
      "source": [
        "f_test = open('test.txt', 'w+')\n",
        "for rev in reviews_test:\n",
        "  df_relevant = df_new_test[df_new_test['Review'] == rev]\n",
        "  for i in range(len(df_relevant)):\n",
        "    f_test.write(str(df_relevant['Tokens'].iloc[i]) + '\\t' + str(df_relevant['combined_labels'].iloc[i] + '\\n'))\n",
        "\n",
        "  f_test.write('\\n')\n",
        "f_test.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FHQ7OKbRHqm4"
      },
      "source": [
        "#### Processing and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ODwZeMRHqm2",
        "colab": {}
      },
      "source": [
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"bert-base-german-cased\",\n",
        "    do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p3dM_kxcHqmz",
        "colab": {}
      },
      "source": [
        "# This processor will preprocess the data for the CoNLL03 NER task\n",
        "\n",
        "ner_processor = NERProcessor(tokenizer=tokenizer, \n",
        "                             max_seq_len=512, \n",
        "                             data_dir=\"\",\n",
        "                             train_filename='train.txt',\n",
        "                             dev_filename=None\n",
        "                             #dev_split=0.1\n",
        "                             )\n",
        "\n",
        "ner_labels = ['B-Claim', 'I-Claim', 'B-Premise', 'I-Premise', 'B-O', 'X', '[PAD]']\n",
        "ner_processor.add_task(\"ner\", \"seq_f1\", ner_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1vrmhq1Hqmv",
        "colab": {}
      },
      "source": [
        "# This prediction head is also a feed forward neural network but expects one\n",
        "# vector per token in the input sequence and will generate a set of logits\n",
        "# for each input\n",
        "\n",
        "LAYER_DIMS = [768, 7]\n",
        "\n",
        "ner_prediction_head = TokenClassificationHead(layer_dims=LAYER_DIMS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOEyPixXHqmt",
        "colab": {}
      },
      "source": [
        "# We can integrate these new pieces with the rest using this code\n",
        "# It is pretty much the same structure as what we had above for text classification\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "EMBEDS_DROPOUT_PROB = 0.1\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_PROPORTION = 0.1\n",
        "N_EPOCHS = 3\n",
        "N_GPU = 1\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=ner_processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[ner_prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_token\"],\n",
        "    device=device)\n",
        "\n",
        "optimizer, warmup_linear = initialize_optimizer(\n",
        "    model=model,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_proportion=WARMUP_PROPORTION,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "trainer = Trainer(\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    warmup_linear=warmup_linear,\n",
        "    device=device,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6mg0Q3e7Hqmr",
        "colab": {}
      },
      "source": [
        "model = trainer.train(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q7Fj7zuwT4Wo"
      },
      "source": [
        "## Model Evaluation Direct Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CbP_PQvKUI-I"
      },
      "source": [
        "### Data handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxos0p20Udh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#New Preparation of the dataset\n",
        "df_new = df\n",
        "\n",
        "z='O'\n",
        "combined_labels = list()\n",
        "for i in range(len(df_new)):\n",
        "  if df_new['BIO'].iloc[i] == 'B':\n",
        "    z = df_new['Ann_Ann'].iloc[i]\n",
        "    combined_labels.append('B-B' + z)\n",
        "  elif df_new['BIO'].iloc[i] == 'I':\n",
        "    combined_labels.append('B-I' + z)\n",
        "  else:\n",
        "    z = 'O'\n",
        "    combined_labels.append('B-' + z)\n",
        "\n",
        "df_new['combined_labels'] = combined_labels\n",
        "\n",
        "#Split of the train and test data\n",
        "reviews_train, reviews_test = train_test_split(reviews, test_size=0.2, random_state=42)\n",
        "df_new_train = df_new[df_new['Review'].isin(reviews_train)]\n",
        "df_new_test = df_new[df_new['Review'].isin(reviews_test)]\n",
        "\n",
        "print(df_new.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JPOxSgUNUI-X",
        "colab": {}
      },
      "source": [
        "#def write_inputfiles_txt(df=[df_new_train, df_new_dev, df_new_test], reviews=[reviews_train, reviews_dev, reviews_test], part=['train', 'dev', 'test']):\n",
        "def write_inputfiles_txt(df=[df_new_train, df_new_test], reviews=[reviews_train, reviews_test], part=['train', 'test']):\n",
        "  r = 0\n",
        "  for df_i in df:\n",
        "    f = open('{}.txt'.format(part[r]), 'w+')\n",
        "    for rev in reviews[r]:\n",
        "      df_relevant = df_i[df_i['Review'] == rev]\n",
        "      for i in range(len(df_relevant)):\n",
        "        f.write(str(df_relevant['Tokens'].iloc[i]) + '\\t' + str(df_relevant['combined_labels'].iloc[i]))\n",
        "        f.write('\\n')\n",
        "      f.write('\\n')\n",
        "    f.close()\n",
        "    r += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gMiY8_-0UI-Z",
        "colab": {}
      },
      "source": [
        "write_inputfiles_txt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eVrwEzLUI-b",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "#Save the files to drive\n",
        "\n",
        "files.download('train.txt')\n",
        "files.download('test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akuiYxSGYxmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from farm.experiment import run_experiment, load_experiments\n",
        "experiment_path = F\"/content/gdrive/My Drive/Colab Notebooks/Corpus/experiments/Direct/arguEval_direct.json\"\n",
        "experiments = load_experiments(experiment_path)\n",
        "f = open('logbook.txt', 'w+')\n",
        "for i in range(len(experiments)):\n",
        "  start = time.time()\n",
        "  run_experiment(experiments[i])\n",
        "  end = time.time()\n",
        "  f.write('Experiment {}: runtime: {}'.format(i, end-start))\n",
        "  f.write('\\n')\n",
        "f.close()\n",
        "files.download('logbook.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kI1usZaY7qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wG8lcleiTbQ",
        "colab_type": "text"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S1hGKJB5iXM2"
      },
      "source": [
        "### Data handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7Mff7-aiXM3",
        "colab": {}
      },
      "source": [
        "#New Preparation of the dataset\n",
        "df_new = df\n",
        "\n",
        "z='O'\n",
        "combined_labels = list()\n",
        "for i in range(len(df_new)):\n",
        "  if df_new['BIO'].iloc[i] == 'B':\n",
        "    z = df_new['Ann_Ann'].iloc[i]\n",
        "    combined_labels.append('B-B' + z)\n",
        "  elif df_new['BIO'].iloc[i] == 'I':\n",
        "    combined_labels.append('B-I' + z)\n",
        "  else:\n",
        "    z = 'O'\n",
        "    combined_labels.append('B-' + z)\n",
        "\n",
        "df_new['combined_labels'] = combined_labels\n",
        "\n",
        "#Split of the train and test data\n",
        "reviews_train, reviews_dev_test = train_test_split(reviews, test_size=0.3, random_state=42)\n",
        "reviews_dev, reviews_test = train_test_split(reviews_dev_test, test_size=(1/3), random_state=42)\n",
        "df_new_train = df_new[df_new['Review'].isin(reviews_train)]\n",
        "df_new_dev = df_new[df_new['Review'].isin(reviews_dev)]\n",
        "df_new_test = df_new[df_new['Review'].isin(reviews_test)]\n",
        "\n",
        "print(df_new.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFs40o-ViXM6",
        "colab": {}
      },
      "source": [
        "def write_inputfiles_txt(df=[df_new_train,df_new_dev, df_new_test], reviews=[reviews_train, reviews_dev, reviews_test], part=['train1','dev1', 'test1']):\n",
        "  r = 0\n",
        "  for df_i in df:\n",
        "    f = open('{}.txt'.format(part[r]), 'w+')\n",
        "    for rev in reviews[r]:\n",
        "      df_relevant = df_i[df_i['Review'] == rev]\n",
        "      for i in range(len(df_relevant)):\n",
        "        f.write(str(df_relevant['Tokens'].iloc[i]) + '\\t' + str(df_relevant['combined_labels'].iloc[i]))\n",
        "        f.write('\\n')\n",
        "      f.write('\\n')\n",
        "    f.close()\n",
        "    r += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz84KSninIdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_inputfiles_txt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUHDP9xcl514",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#new writing approach --> NOT WORKING\n",
        "\"\"\"\n",
        "def write_inputfiles_txt(df, reviews, part):\n",
        "  f = open('{}.txt'.format(part), 'w+')\n",
        "  for rev in reviews[r]:\n",
        "    df_relevant = df_i[df_i['Review'] == rev]\n",
        "    for i in range(len(df_relevant)):\n",
        "      f.write(str(df_relevant['Tokens'].iloc[i]) + '\\t' + str(df_relevant['combined_labels'].iloc[i]))\n",
        "      f.write('\\n')\n",
        "  f.write('\\n')\n",
        "  f.close()\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PsQWP_VZiXM8",
        "colab": {}
      },
      "source": [
        "\"\"\"write_inputfiles_txt(df_new_train, reviews_train, 'train')\n",
        "write_inputfiles_txt(df_new_dev, reviews_dev, 'dev')\n",
        "write_inputfiles_txt(df_new_test, reviews_test, 'test')\"\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MQSZwloAiXM_",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "#Save the files to drive\n",
        "\n",
        "files.download('train.txt')\n",
        "files.download('dev.txt')\n",
        "files.download('test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccKuEu8i752D",
        "colab_type": "text"
      },
      "source": [
        "### run experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EwXDpCneiXNE",
        "colab": {}
      },
      "source": [
        "\n",
        "from farm.experiment import run_experiment, load_experiments\n",
        "experiment_path = F\"/content/gdrive/My Drive/Colab Notebooks/Corpus/experiments/Direct/arguEval_direct_new_new.json\"\n",
        "experiments = load_experiments(experiment_path)\n",
        "\n",
        "for i in range(len(experiments)):\n",
        "  run_experiment(experiments[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZC-zDkfQWmou"
      },
      "source": [
        "## New Best Model\n",
        "learningrate = 5e-5;\n",
        "\n",
        "warumup_proportion = 0.1;\n",
        "\n",
        "embedding_dropout_prob = 0.15\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZLjf_cjWmo5",
        "colab": {}
      },
      "source": [
        "from farm.experiment import run_experiment, load_experiments\n",
        "final_model_path = F\"/content/gdrive/My Drive/Colab Notebooks/Corpus/experiments/Direct/final_model.json\"\n",
        "final_model = load_experiments(final_model_path)\n",
        "\n",
        "run_experiment(final_model[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "USxhw7QHWmpB",
        "colab": {}
      },
      "source": [
        "from farm.experiment import run_experiment, load_experiments\n",
        "final_model_path = F\"/content/gdrive/My Drive/Colab Notebooks/Corpus/experiments/Direct/final_model_premise_claim.json\"\n",
        "final_model = load_experiments(final_model_path)\n",
        "\n",
        "run_experiment(final_model[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ge50NQV1WmpG"
      },
      "source": [
        "### Data handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HyTBbI5OWmpH",
        "colab": {}
      },
      "source": [
        "#New Preparation of the dataset\n",
        "df_new = df\n",
        "\n",
        "z='O'\n",
        "combined_labels = list()\n",
        "for i in range(len(df_new)):\n",
        "  if df_new['BIO'].iloc[i] == 'B':\n",
        "    z = df_new['Ann_Ann'].iloc[i]\n",
        "    combined_labels.append('B-' + z)\n",
        "  elif df_new['BIO'].iloc[i] == 'I':\n",
        "    combined_labels.append('I-' + z)\n",
        "  else:\n",
        "    z = 'O'\n",
        "    combined_labels.append(z)\n",
        "\n",
        "df_new['combined_labels'] = combined_labels\n",
        "\n",
        "#Split of the train and test data\n",
        "reviews_train, reviews_dev_test = train_test_split(reviews, test_size=0.3, random_state=42)\n",
        "reviews_dev, reviews_test = train_test_split(reviews_dev_test, test_size=(1/3), random_state=42)\n",
        "df_new_train = df_new[df_new['Review'].isin(reviews_train)]\n",
        "df_new_dev = df_new[df_new['Review'].isin(reviews_dev)]\n",
        "df_new_test = df_new[df_new['Review'].isin(reviews_test)]\n",
        "\n",
        "print(df_new.head(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pox0WbYVWmpK",
        "colab": {}
      },
      "source": [
        "def write_inputfiles_txt(df=[df_new_train,df_new_dev, df_new_test], reviews=[reviews_train, reviews_dev, reviews_test], part=['train1','dev1', 'test1']):\n",
        "  r = 0\n",
        "  for df_i in df:\n",
        "    f = open('{}.txt'.format(part[r]), 'w+')\n",
        "    for rev in reviews[r]:\n",
        "      df_relevant = df_i[df_i['Review'] == rev]\n",
        "      for i in range(len(df_relevant)):\n",
        "        f.write(str(df_relevant['Tokens'].iloc[i]) + '\\t' + str(df_relevant['combined_labels'].iloc[i]))\n",
        "        f.write('\\n')\n",
        "      f.write('\\n')\n",
        "    f.close()\n",
        "    r += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wi988XsiWmpM",
        "colab": {}
      },
      "source": [
        "write_inputfiles_txt()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L9xr4GByWmpQ",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "#Save the files to drive\n",
        "\n",
        "files.download('train1.txt')\n",
        "files.download('dev1.txt')\n",
        "files.download('test1.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d7grP-0CWmpT",
        "colab": {}
      },
      "source": [
        "from farm.experiment import run_experiment, load_experiments\n",
        "final_model_path = F\"/content/gdrive/My Drive/Colab Notebooks/Corpus/experiments/Direct/final_model_premise_claim.json\"\n",
        "final_model = load_experiments(final_model_path)\n",
        "\n",
        "run_experiment(final_model[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}